# -*- coding: utf-8 -*-
"""DecisionTrees_FakeNews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gv0QA4chYeGQqwWq6Ar7cPH8Crj-vKqd

# Decision Trees

https://tinyurl.com/52y9xjkd
"""

#Read and print the data.csv dataset
import pandas

df = pandas.read_csv("data.csv")

print(df)

"""To make a decision tree, all data has to be numerical.

We have to convert the non numerical columns 'Nationality' and 'Go' into numerical values.

Pandas has a map() method that takes a dictionary with information on how to convert the values.

{'UK': 0, 'USA': 1, 'N': 2}

Means convert the values 'UK' to 0, 'USA' to 1, and 'N' to 2.
"""

#Change string values into numerical values:
d = {'UK':0, 'USA':1, 'N':2}
df['Nationality'] = df['Nationality'].map(d)
d = {'YES':1, 'NO':0}
df['Go'] = df['Go'].map(d)
print(df)

"""Then we have to separate the feature columns from the target column.

The feature columns are the columns that we try to predict from, and the target column is the column with the values we try to predict.
"""

#X is the feature columns, y is the target column:
features = ['Age', 'Experience', 'Rank', 'Nationality']

X = df[features]
y = df['Go']

print(X)
print(y)

#Create and display a Decision Tree:
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

dtree_object = DecisionTreeClassifier()
dtree_model = dtree_object.fit(X,y)

tree.plot_tree(dtree_model, feature_names = features)

"""## Result Explained
The decision tree uses your earlier decisions to calculate the odds for you to wanting to go see a comedian or not.

Let us read the different aspects of the decision tree:

Rank <= 6.5 means that every comedian with a rank of 6.5 or lower will follow the True arrow (to the left), and the rest will follow the False arrow (to the right).

gini = 0.497 refers to the quality of the split, and is always a number between 0.0 and 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.

samples = 13 means that there are 13 comedians left at this point in the decision, which is all of them since this is the first step.

value = [6, 7] means that of these 13 comedians, 6 will get a "NO", and 7 will get a "GO".

The next step contains two boxes, one box for the comedians with a 'Rank' of 6.5 or lower, and one box with the rest.

### True - 5 Comedians End Here:
gini = 0.0 means all of the samples got the same result.

samples = 5 means that there are 5 comedians left in this branch (5 comedian with a Rank of 6.5 or lower).

value = [5, 0] means that 5 will get a "NO" and 0 will get a "GO".

###False - 8 Comedians Continue:

Nationality <= 0.5 means that the comedians with a nationality value of less than 0.5 will follow the arrow to the left (which means everyone from the UK, ), and the rest will follow the arrow to the right.

gini = 0.219 means that about 22% of the samples would go in one direction.

samples = 8 means that there are 8 comedians left in this branch (8 comedian with a Rank higher than 6.5).

value = [1, 7] means that of these 8 comedians, 1 will get a "NO" and 7 will get a "GO".


... AND SO ON

## Predict Values
We can use the Decision Tree to predict new values.

Example: Should I go see a show starring a 40 years old American comedian, with 10 years of experience, and a comedy ranking of 7?
"""

#Use predict() method to predict new values:
print(dtree_model.predict([[40, 10, 7, 1]]))

"""# Cleaning News Data
We need to clean the news data so it is usable by the decision tree algorithm/model.
"""

#unzip fake news data
!unzip "News_dataset.zip"

#Load the data
df = pandas.read_csv("Fake.csv")
print(df.head())

"""## Bag of Words
### Step 1: Collect Data
Below is a snippet of the first few lines of text from the book “A Tale of Two Cities” by Charles Dickens, taken from Project Gutenberg.

It was the best of times,
it was the worst of times,
it was the age of wisdom,
it was the age of foolishness,

For this small example, let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.

###Step 2: Design the Vocabulary
Now we can make a list of all of the words in our model vocabulary.

The unique words here (ignoring case and punctuation) are:


*   "it"
*   "was"
* "the"
* "best"
* "of"
* "times"
* "worst"
* "age"
* "wisdom"
* "foolishness"

That is a vocabulary of 10 words from a corpus containing 24 words.

###Step 3: Create Document Vectors
The next step is to score the words in each document.

The objective is to turn each document of free text into a vector that we can use as input or output for a machine learning model.

Because we know the vocabulary has 10 words, we can use a fixed-length document representation of 10, with one position in the vector to score each word.

The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.

Using the arbitrary ordering of words listed above in our vocabulary, we can step through the first document (“It was the best of times“) and convert it into a binary vector.

The scoring of the document would look as follows:

*   "it" = 1
*   "was" = 1
* "the" = 1
* "best" = 1
* "of" = 1
* "times" = 1
* "worst" = 0
* "age" = 0
* "wisdom" = 0
* "foolishness" = 0

As a binary vector, this would look as follows:
```
[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
```

The other three documents would look as follows:

```
"it was the worst of times" = [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]
"it was the age of wisdom" = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]
"it was the age of foolishness" = [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]
```
"""

def clean_article(text):
  #remove punctuation
  text = text.lower()
  text = text.replace('.','')
  text = text.replace(',','')
  text = text.replace('!','')
  text = text.replace('"','')
  text = text.replace("'",'')
  text = text.replace('?','')
  text = text.replace(':','')
  text = text.replace('/','')
  text = text.replace('@','')
  text = text.replace('(','')
  text = text.replace(')','')
  text = text.replace('[','')
  text = text.replace(']','')
  text = text.replace('_','')
  text = text.replace('*','')
  text = text.replace('0','')
  text = text.replace('1','')
  text = text.replace('2','')
  text = text.replace('3','')
  text = text.replace('4','')
  text = text.replace('5','')
  text = text.replace('6','')
  text = text.replace('7','')
  text = text.replace('8','')
  text = text.replace('9','')
  text = text.replace('-','')
  text = text.replace('#','')
  text = text.replace(';','')

  #split into words
  text = text.strip().split()

  #remove links
  text = [ x for x in text if "www" not in x ]
  text = [ x for x in text if "http" not in x ]

  return text

"""Use the code below to extend to clean real articles and get the vocab from the real articles.
You want a combined vocabulary for real and fake articles.

THEN
You need to process the real articles like you did with the fake articles to create the BOW vectors.

Write out BOW vectors for real and fake to a new CSV (news_data.csv). The final column should be the label (FAKE=1, REAL = 0)

FINALLY
Train and test a Decision Tree
"""

#Turn Fake and Real news into bag of words (use the text from the article)

#You will need to use both sets of data to create a common vocabulary

#I recommend using a dictionary, rather than a list, to keep track of your BOW

#Save your data in a new CSV file (news_data.csv) that has the vocabulary as
#the header and the counts as the features for each article

#you want to remove words that are infrequent. Choose a threshold like 5 or 10 to remove misspellings/uncommon words

#Use the first 1000 real and fake document to create your vocabulary

import pandas as pd

df_fake = pd.read_csv("Fake.csv")

df_fake = df_fake['text']

word_dict = {}

#get the vocabulary words for the fake news data
cnt = 0 #count the first 1000 articles
for text in df_fake: #iterate through every article in the fake data frame
  text = clean_article(text)
  for word in text:
    try:
      word_dict[word] += 1
    except:
      word_dict[word] = 1 #counting words in the fake news articles

  cnt += 1
  if cnt > 1000:
    break

#insert code here to add real article vocabulary

#remove words that occur less than min_thresh and more than max_thresh times
min_thresh = 5
max_thresh = 1000

vocab = list(word_dict)
print("Vocab Size Before Removal:", len(vocab))

for word in vocab:
  if word_dict[word] < min_thresh or word_dict[word] > max_thresh:
    word_dict.pop(word)

vocab = list(word_dict)
print("Vocab Size After Removal:", len(vocab))

#turn each article in the fake news set into a bag of words vector

#create an empty article dictionary
article_dict = word_dict.copy()
article_dict = dict.fromkeys(article_dict, 0) #setting all counts in article_dict to 0

#create bow for each article
cnt = 0
for text in df_fake:
  text = clean_article(text)
  for word in text:
    try:
      article_dict[word] += 1 #get count of words in article
    except:
      continue

  #article dictionary, turn into an list which is our bow for the article
  article_list = list(article_dict.values())

  #turn this into a string with comma separated values so you can write it out to a CSV
  #REMEMBER TO add a label at the end (FAKE = 1, REAL = 0)

  #zero out the article dictionary counts
  article_dict = dict.fromkeys(article_dict, 0)

  #only keep the first 1000 articles
  cnt += 1
  if cnt > 1000:
    break

#repeat this process of cleaning, creating bow vectors for the real data.

def MinningWords(FileName,First_Article,Last_Article):
  #Load the data
  import pandas
  import numpy as np
  #unzip fake news data
  #!unzip "News _dataset.zip"
  df = pandas.read_csv(FileName)

  #Invetigating the words with most repetition
  Mined_words={}
  for line in (df.title[First_Article:Last_Article]):
    text = clean_article(line)
    for word in text:
      if word not in Mined_words:
        Mined_words[word]=1
      else:
        Mined_words[word]+=1
  #Filtering data
  Min_Threshold=10
  Max_Threshold=1000
  Mined_words_filtered={}
  for word in Mined_words:
    if Mined_words[word]>Min_Threshold and Mined_words[word]<Max_Threshold:
      Mined_words_filtered[word]=Mined_words[word]
  #copying keys to an array
  Mined_words_filtered_list=[]
  for word in  Mined_words_filtered:
    Mined_words_filtered_list.append(word)

  return (Mined_words_filtered_list)

Mined_words_filtered_list_Fake=MinningWords("Fake.csv",0,1000)
Mined_words_filtered_list_True=MinningWords("True.csv",0,1000)

#Merging
merged_list=Mined_words_filtered_list_Fake
for item in Mined_words_filtered_list_True:
   if item not in merged_list:
    merged_list.append(item)

#Having the number of times that nominated words are showed up in each article
def repeatitaion_chart(First_Article,Last_Article,words_list,FileName):
  df = pandas.read_csv(FileName)
  length=Last_Article-First_Article
  repeatitaion_chart=np.zeros((length, len(words_list)))
  index_X=0
  for line in df.title[First_Article:Last_Article]:
    cleansed_line=clean_article(line)
    for word in cleansed_line:
      if word in words_list:
        index_Y = words_list.index(word)
        repeatitaion_chart[index_X,index_Y]+=1
    index_X+=1
  return repeatitaion_chart

chart_Fake=repeatitaion_chart(0,1000,merged_list,'Fake.csv')
Label=np.zeros((chart_Fake.shape[0],1))
chart_Fake=np.hstack((chart_Fake, Label))

chart_True=repeatitaion_chart(0,1000,merged_list,'True.csv')
Label=np.ones((chart_True.shape[0],1))
chart_True=np.hstack((chart_True, Label))
Train_merged_list=merged_list.copy()
Train_merged_list.append("label")
#Puting the tables together
bow_list=np.vstack([chart_Fake,chart_True])
bow_list=np.vstack([Train_merged_list,bow_list])
#CSV output
np.savetxt("Train_data.csv", bow_list, delimiter=",", fmt="%s")

#Decision Tree and trainig
data = pandas.read_csv('Train_data.csv')
#Features and Target
features_train = data.iloc[:,:-1]
target_train = data['label']

#Create and display a Decision Tree:
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt

dtree_object = DecisionTreeClassifier()
dtree_model = dtree_object.fit(features_train,target_train)

tree.plot_tree(dtree_model, feature_names = list(features_train.columns))

#Preparing Test data and predcition
Test_Fake=repeatitaion_chart(1001,2001,merged_list,'Fake.csv')
Label=np.zeros((Test_Fake.shape[0],1))
Test_Fake=np.hstack((Test_Fake, Label))

Test_True=repeatitaion_chart(1001,2001,merged_list,'True.csv')
Label=np.ones((Test_True.shape[0],1))
Test_True=np.hstack((Test_True, Label))
Testmerged_list=merged_list.copy()
Testmerged_list.append("label")

#Puting the tables together
Test_bow_list=np.vstack([Test_Fake,Test_True])
Test_bow_list=np.random.permutation(Test_bow_list)#Shuffeling
Test_bow_list=np.vstack([Testmerged_list,Test_bow_list])
#Test_bow_list=np.random.shuffle(Test_bow_list)

#CSV output
np.savetxt("Test_data.csv", Test_bow_list, delimiter=",", fmt="%s")

#Prediction
from sklearn.metrics import accuracy_score
#Decision Tree
Test_data = pandas.read_csv('Test_data.csv')
#Features and Target
features_test = Test_data.iloc[:,:-1]
target_test = Test_data['label']

y_pred = dtree_object.predict(features_test)
accuracy = accuracy_score(target_test, y_pred)
print("Accuracy: ", accuracy)



